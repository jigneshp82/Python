{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Non-linear encoder</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<b>In this lab, we compare the ability of non-linear encoders to represent features better than linear encoders. This has applications in data visualization and clustering. We will illustrate this by visualizing the features of the 255-dimensional MIST dataset in a three-dimensional latent space.  We will see in the latent space that the samples are much better clustered with respect to each class using the Tanh function compared to the linear activation function. </p>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"#prep\">Preparation</a></li>\n",
    "<li><a href=\"#Load_Data\">Load Data</a></li>\n",
    "<li><a href=\"#AE\">Differnt type of Autoencders/a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"prep\" >Preparation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries we need to use in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Pillow==6.2.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/bbbc569f98f47813c50a116b539d97b3b17a86ac7a309f83b2022d26caf2/Pillow-6.2.2-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 1.2MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Pillow\n",
      "  Found existing installation: Pillow 8.2.0\n",
      "    Uninstalling Pillow-8.2.0:\n",
      "      Successfully uninstalled Pillow-8.2.0\n",
      "Successfully installed Pillow-6.2.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4e141617d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the following line code to install the torchvision library\n",
    "# !conda install -y torchvision\n",
    "!pip install Pillow==6.2.2\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as colors\n",
    "torch.manual_seed(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the models form object storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/DL0110EN/Version_3/Chapter_10/linear_visualization_MIST.pt: No such file or directory\n",
      "/bin/bash: https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/DL0110EN/Version_3/Chapter_10/tan_linear_out_visualization_MIST.pt: No such file or directory\n",
      "/bin/bash: https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/DL0110EN/Version_3/Chapter_10/auto_encoder_tanh_in_out.pt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/DL0110EN/Version_3/Chapter_10/linear_visualization_MIST.pt\n",
    "!https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/DL0110EN/Version_3/Chapter_10/tan_linear_out_visualization_MIST.pt\n",
    "!https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/DL0110EN/Version_3/Chapter_10/auto_encoder_tanh_in_out.pt   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will plot a PyTorch tensor as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(data_sample, y=None):\n",
    "    plt.imshow(data_sample[0].detach().numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will plot the training cost and the validation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val(cost_list,accuracy_list,val_data_label ='accuracy'):\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(cost_list, color = color)\n",
    "    ax1.set_xlabel('epoch ', color = color)\n",
    "    ax1.set_ylabel('total loss', color = color)\n",
    "    ax1.tick_params(axis = 'y', color = color)\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(val_data_label, color = color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(accuracy_list, color = color)\n",
    "    ax2.tick_params(axis = 'y', color = color)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will plot the output of the encoder of an autoencoder and colour code them with respect to the class of the MIST dataset. The activation must be 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plot_activations_3d():\n",
    "    def __init__(self,model_,layer_name,dataset,tensor_size=256):\n",
    "        \"\"\"Plot activations of a  neal network and colour cots them with respect to class activaion must be 3D \n",
    "\n",
    "            arguments:\n",
    "                model_:neral network model \n",
    "                layer_name:name of activation layer, must be  2D or 3D\n",
    "                dataset:dataset object \n",
    "                tensor_size:size of input tensor       \n",
    "\n",
    "            attributes:\n",
    "                self.Y: class of each sample\n",
    "                self.Z: (np.array): array of state-values obtained from policy evaluation function.\n",
    "          \n",
    "        \"\"\"\n",
    "        self.L=len(dataset)\n",
    "        self.Z=np.ones((len(dataset),3))\n",
    "        self.Y=np.ones(len(dataset))\n",
    "        for n,(x,y) in enumerate(dataset): \n",
    "            z=torch.squeeze(getattr(model_,layer_name)(x.view(-1,tensor_size))).detach().numpy()\n",
    "            self.Z[n,:]=z\n",
    "            self.Y[n]=y.numpy()    \n",
    "\n",
    "    def plot(self, numbers=[0,1,2,3,4,5,6,7,8,9],scale=[1,1,1]):\n",
    "        \"\"\"\n",
    "        number: list of classes to be plotted \n",
    "        scale: scale activations to plot better \n",
    "        \"\"\"\n",
    "        color_list=['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w','bx','rx']\n",
    "        logic_array =np.zeros(self.L, dtype=bool)\n",
    "        color_array=np.chararray(self.L,unicode=True)\n",
    "\n",
    "        for number in numbers:\n",
    "            logic_array=logic_array+(self.Y==number)\n",
    "            color_array[self.Y==number]=color_list[number]\n",
    "\n",
    "\n",
    "        fig=plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        plt.scatter(scale[0]*self.Z[logic_array,0],scale[1]*self.Z[logic_array,1],scale[2]*self.Z[logic_array,2],c=color_array)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function trains the autoencoder; the parameter <code>model</code> is the autoencoder object. The parameter  <code>train_loader</code> and <code>validation_loader</code> is the train loader and validation loader.  The Parameter optimizer is the optimizer object, and <code>n_epoch</code> is the number of epochs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4,checkpoint_path=None,checkpoint=None):   \n",
    "    #global variable \n",
    "    cost_list_training =[]\n",
    "    cost_list_validation =[]\n",
    "    for epoch in range(n_epochs):\n",
    "        cost_training=0\n",
    "        for x, y in train_loader:\n",
    "           \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x.view(-1,256))\n",
    "            loss = criterion(z, x.view(-1,256))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cost_training+=loss.data\n",
    "        \n",
    "        cost_list_training.append(cost_training)\n",
    "    \n",
    "       \n",
    "        print(\"epoch {}, Cost {}\".format(epoch+1,cost_training) )\n",
    "        #perform a prediction on the validation  data  \n",
    "        cost_val=0\n",
    "        for x_test, y_test in validation_loader:\n",
    "            \n",
    "            model.eval()\n",
    "            z = model(x_test.view(-1,256))\n",
    "            loss = criterion(z, x_test.view(-1,256))\n",
    "            cost_val+=loss.data\n",
    "            \n",
    "            \n",
    "        \n",
    "        cost_list_validation.append(cost_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "    if checkpoint:\n",
    "        checkpoint['epoch']=epoch\n",
    "        checkpoint['model_state_dict']=model.state_dict()\n",
    "        checkpoint['optimizer_state_dict']= optimizer.state_dict()\n",
    "        checkpoint['loss']=loss \n",
    "        checkpoint['training_cost']=cost_list_training\n",
    "        checkpoint['validaion_cost']=cost_list_validation\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "     \n",
    "    return cost_list_training, cost_list_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Load\"> Load Data</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a transform to resize the image and convert it to a tensor :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "tensor_size=IMAGE_SIZE*IMAGE_SIZE\n",
    "composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset by setting the parameters <code>train </code> to <code>True</code>. We use the transform defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset by setting the parameters train  <code>False</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Make the validating \n",
    "\n",
    "validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data type is long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the data type for each element in dataset\n",
    "train_dataset[0][1].type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.2.1imagenet.png\" width=\"550\" alt=\"MNIST data image\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the fourth label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The label for the fourth data element\n",
    "\n",
    "train_dataset[3][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fourth sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The image for the fourth data element\n",
    "show_data(validation_dataset[77])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a train loader and data loader  object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"AE\">Differnt type of Autoencders </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we build autoencoders with different encoding and decoding functions and compare them. To save time  rather than training the model, you can load models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an auto-encoder class or custom module with one layer.  The parameter <code>D_in </code> is the input dimension, the parameter <code>H </code> is the number of hidden neurons or nodes or the dimension of the encoding dimensions.  This will transform the input into the latent space. The output is sometimes called the code.  The output dimension is the same as the input dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an autoencoder  object the encoding dimension will be three we will visualize the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Linear Autoencoder  </b>\n",
    "<p>\n",
    "We have a linear autoencoder, module or class: \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, input_dim=256, encoding_dim=32):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Linear(input_dim,encoding_dim)\n",
    "        self.decoder = nn.Linear(encoding_dim,input_dim)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x =  self.encoder(x)\n",
    "        \n",
    "        x=self.decoder(x)\n",
    "      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an autoencoder object, criterion function and optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder_3  = AutoEncoder(input_dim=tensor_size , encoding_dim=3)\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(auto_encoder_3.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the Root means square error,ff you don't want to train the model, you can load it in the next cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint={'epoch':None,'model_state_dict':None ,'optimizer_state_dict':None ,'loss': None ,'training_cost':None,'validaion_cost':None }\n",
    "checkpoint_path='linear_visualization_MIST.pt'\n",
    "cost_list_training, cost_list_validation=train_model(auto_encoder_3 ,train_loader,validation_loader,optimizer,n_epochs=10,checkpoint_path=checkpoint_path,checkpoint=checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'linear_visualization_MIST.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-53bfb00e9fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear_visualization_MIST.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mauto_encoder_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'linear_visualization_MIST.pt'"
     ]
    }
   ],
   "source": [
    "checkpoint_path='linear_visualization_MIST.pt'\n",
    "checkpoint= torch.load(checkpoint_path)\n",
    "auto_encoder_3.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the cost on the training and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d2d8acfac2c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcost_list_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_list_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training_cost'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validaion_cost'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_train_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_list_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_list_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_data_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Cost '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "cost_list_training,cost_list_validation=checkpoint['training_cost'], checkpoint['validaion_cost']\n",
    "plot_train_val(cost_list_training,cost_list_validation,val_data_label ='Validation Cost ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can output  the code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=auto_encoder_3.encoder(validation_dataset[5][0].view(-1,256))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can make a prediction, the output is the same shape as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat=auto_encoder_3(validation_dataset[5][0].view(-1,256))\n",
    "print(xhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the class of the input is a one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset[5][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the output it looks like a one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(xhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot out the output of the enoding dimension or code, and compare it to the activations of the hidden layer of the neural network. Although there is some overlap, we see that similar to the neural network each cluster corresponds to each class. This result is obtained even though there are no labels in the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_3_act=plot_activations_3d(model_=auto_encoder_3 ,layer_name='encoder',dataset=train_dataset,tensor_size=tensor_size)\n",
    "auto_3_act.plot(scale=[100,100,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Tanh Encoding Function</b>\n",
    "\n",
    "Autoencoder module or class with Tanh Encoding function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderTanh(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, input_dim=256, encoding_dim=32):\n",
    "        super(AutoEncoderTanh, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Linear(input_dim,encoding_dim)\n",
    "        self.decoder = nn.Linear(encoding_dim,input_dim)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x =  torch.tanh(self.encoder(x))\n",
    "        \n",
    "        x=self.decoder(x)\n",
    "      \n",
    "        return x\n",
    "    def code(self,x):\n",
    "        return torch.tanh(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an autoencoder object, criterion function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder_tanh = AutoEncoderTanh(input_dim=tensor_size , encoding_dim=3)\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(auto_encoder_tanh.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the Root means square error,ff you don't want to train the model, you can load it in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint={'epoch':None,'model_state_dict':None ,'optimizer_state_dict':None ,'loss': None ,'training_cost':None,'validaion_cost':None }\n",
    "checkpoint_path='tan_linear_out_visualization_MIST.pt'\n",
    "cost_list_training, cost_list_validation=train_model(auto_encoder_tanh ,train_loader,validation_loader,optimizer,n_epochs=10,checkpoint_path=checkpoint_path,checkpoint=checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path='tan_linear_out_visualization_MIST.pt'\n",
    "checkpoint= torch.load(checkpoint_path)\n",
    "auto_encoder_tanh.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the cost on the training and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_list_training,cost_list_validation=checkpoint['training_cost'], checkpoint['validaion_cost']\n",
    "plot_train_val(cost_list_training,cost_list_validation,val_data_label ='Validation Cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can obtain the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=auto_encoder_tanh.encoder(validation_dataset[5][0].view(-1,256))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can make a prediction, the output is the same shape as the input convert it to a rectangle image a view it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat=auto_encoder_tanh(validation_dataset[5][0].view(-1,256))\n",
    "show_data(xhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot out the output of the latent  dimension or code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_3_act=plot_activations_3d(model_=auto_encoder_tanh ,layer_name='encoder',dataset=train_dataset,tensor_size=tensor_size)\n",
    "auto_3_act.plot(scale=[100,100,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that all the clusters are segmented with respect to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Autoencoder Tanh Encoding and Decoding Function </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an Autoencoder module or class with Tanh Encoding and Decoding function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderTanhDecoder(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, input_dim=256, encoding_dim=32):\n",
    "        super(AutoEncoderTanhDecoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Linear(input_dim,encoding_dim)\n",
    "        self.decoder = nn.Linear(encoding_dim,input_dim)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x =  torch.tanh(self.encoder(x))\n",
    "        \n",
    "        x=torch.tanh(self.decoder(x))\n",
    "      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the tanh encoding function. We can plot the code; we see the clusters with respect  to each class is more apparent than a linear autoencoder. It’s difficult to say if the clusters are more visible than the autoencoder with the tanh encoding function and linear output.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder_tanh_D = AutoEncoderTanhDecoder(input_dim=tensor_size , encoding_dim=3)\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(auto_encoder_tanh_D.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the Root means a square error,ff you don't want to train the model, you can load it in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint={'epoch':None,'model_state_dict':None ,'optimizer_state_dict':None ,'loss': None ,'training_cost':None,'validaion_cost':None }\n",
    "checkpoint_path='auto_encoder_tanh_in_out.pt'\n",
    "cost_list_training, cost_list_validation=train_model(auto_encoder_tanh_D ,train_loader,validation_loader,optimizer,n_epochs=10,checkpoint_path=checkpoint_path,checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can load the model from memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path='auto_encoder_tanh_in_out.pt'\n",
    "checkpoint= torch.load(checkpoint_path)\n",
    "auto_encoder_tanh_D.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_list_training,cost_list_validation=checkpoint ['training_cost'], checkpoint['validaion_cost']\n",
    "plot_train_val(cost_list_training,cost_list_validation,val_data_label ='Validation Cost')\n",
    "auto_3_act=plot_activations_3d(model_=auto_encoder_tanh_D ,layer_name='encoder',dataset=train_dataset,tensor_size=tensor_size)\n",
    "auto_3_act.plot(scale=[100,100,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with a non-linear autoencoder is the output are not real numbers. For example with the tanh function the output will range from -1 and 1. We can see the output is difficult to see.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat=auto_encoder_tanh_D(validation_dataset[5][0].view(-1,256))\n",
    "show_data(xhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
